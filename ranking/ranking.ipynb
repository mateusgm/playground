{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**data**: query_id F_context [ F_doc(i) ] Doc_choosen\n",
    "\n",
    "\n",
    "map:\n",
    "\n",
    "**hard_rank**: id F_context F_doc | was_choosen\n",
    "\n",
    "**soft_rank**: id F_context F_doc | relevance\n",
    "\n",
    "\n",
    "reduce:\n",
    "\n",
    "**pointwise**:   hard_rank\n",
    "\n",
    "**pointwise++**: hard_rank + F_doc*F_context\n",
    "\n",
    "**pairwise**: id F_context (F_docA - F_docB) (label_A - label_B) [ for different labels ]\n",
    "\n",
    "\n",
    "https://gist.github.com/fabianp/2020955"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import heapq\n",
    "\n",
    "from itertools import izip\n",
    "from sklearn.metrics import *\n",
    "\n",
    "def _d(array, name):\n",
    "    print \"----\", name\n",
    "    for x in array: print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.diag(np.ones(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hash trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "hash_lookup = {}\n",
    "\n",
    "def hash_trick(x, namespaces, n=2**20):\n",
    "    global hash_lookup\n",
    "    sparse_x = []\n",
    "    for row in x:\n",
    "        hashed_x = {}\n",
    "        for v,s in zip(row, namespaces):\n",
    "            my_hash = hash(str(v)+s) % n\n",
    "            hash_lookup[ my_hash ] = \"%s=%s\" % ( s, v ) \n",
    "            hashed_x[ my_hash ]    = 1\n",
    "        sparse_x.append( defaultdict(lambda: 0, hashed_x) )\n",
    "    return sparse_x\n",
    "\n",
    "def interpret_coefs(coefs, n=None):\n",
    "    # TODO\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "from collections import defaultdict\n",
    "\n",
    "def relevance_mapper(X, Y, scorer=None):\n",
    "    scorer = scorer or ( lambda x,y: int(x[-1] == y)*1000000 )\n",
    "    return map(scorer, X, Y)\n",
    "\n",
    "def pairwise_hashed(X, Y, context=None, balanced=True):\n",
    "    X_new, Y_new = [], []\n",
    "    pairs = it.combinations(range(len(X)), 2)\n",
    "\n",
    "    for i, (a,b) in enumerate(pairs):\n",
    "        y  = np.sign(Y[a] - Y[b])\n",
    "        if y == 0: continue\n",
    "        \n",
    "        x = X[a].copy()\n",
    "        for k in X[b].keys(): x[k] -= 1\n",
    "        \n",
    "        if balanced and y != (-1)**i:\n",
    "            y *= -1\n",
    "            for k in x.keys(): x[k] *= -1\n",
    "    \n",
    "        X_new.append(x)\n",
    "        Y_new.append(y)\n",
    "\n",
    "    return X_new, Y_new\n",
    "\n",
    "def to_vw(qid, X,Y):\n",
    "    for x,y in zip(X,Y):\n",
    "        sparse_feat = [ \"%d:%d\" % (k,v) for k,v in x.items() if v ]\n",
    "        print \"{} '{} |p {}\".format(y, qid, \" \".join(sparse_feat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# helpers\n",
    "\n",
    "current_key     = None\n",
    "current_samples = []\n",
    "def process_by_key(x, y, key=None, callback=None, **callback_params):\n",
    "    global current_key, current_samples\n",
    "\n",
    "    if current_key and current_key != key:\n",
    "        callback( current_key, current_samples, **callback_params) \n",
    "        current_samples = [ ]\n",
    "    \n",
    "    current_samples.append( (x, y) )\n",
    "    current_key     = key\n",
    "    \n",
    "\n",
    "def process(qid, rows, pairwise=False):\n",
    "    if qid is None: return\n",
    "\n",
    "    X, Y = zip(*rows)\n",
    "    X_hashed = hash_trick(X, header, n=hash_b)\n",
    "    \n",
    "    if pairwise:\n",
    "        scr  = relevance_mapper(X, Y)\n",
    "        X, Y = pairwise_hashed(X_hashed, scr)\n",
    "    \n",
    "    to_vw(qid, X_hashed, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 'b1 |p 62667:1 611565:1 909087:1\n",
      "-1 'b1 |p 62667:1 31045:1 909087:1\n"
     ]
    }
   ],
   "source": [
    "_file  = open('test.l2b.tsv')\n",
    "header  = _file.readline().strip().split(sep)\n",
    "sep     = \" \"\n",
    "hash_b = 2**20\n",
    "\n",
    "choosen_i = -1 \n",
    "query_i   = 0\n",
    "\n",
    "# notebook fix\n",
    "current_key     = None\n",
    "current_samples = []\n",
    "\n",
    "for line in _file:\n",
    "    row   = line.strip().split(sep)\n",
    "    x = row[query_i+1:choosen_i]\n",
    "    y = row[choosen_i]\n",
    "    process_by_key(x, y, key=row[query_i], callback=process, pairwise=True)\n",
    "\n",
    "process( current_key, current_samples, pairwise=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to_vw -c -pairwise train.csv | vw -i train.csv --link logistic -b 25 -f vw.model\n",
    "# to_vw -c -hashed test.csv    | vw -t test.csv -p | eval labels.csv\n",
    "\n",
    "def truth_mask(truch_doc, all_docs):\n",
    "    np.zeros(len(all_docs))\n",
    "    all_docs[ docs.index(truth_doc) ] = 1\n",
    "    return\n",
    "\n",
    "# how far we need to go to predict the true label?\n",
    "coverage_k = []\n",
    "\n",
    "for i in data:\n",
    "    if current_key == row[key]: continue\n",
    "    process no pairwise\n",
    "    truth  = get chosen doc\n",
    "    y_true = [ int(truth == i) for i in docs_to_consider ]\n",
    "    y_pred = [ predict for i in docs_to_consider ]\n",
    "    k      = coverage_error([ y_true ], [ y_pred ])\n",
    "    coverage_k.append(k)\n",
    "    print np.mean(coverage_k < 3), np.mean(coverage_k < 5), np.mean(coverage_k < 7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing HashTrick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.feature_extraction import *\n",
    "\n",
    "df = pd.read_csv('car.data.csv')\n",
    "x = df[df.columns[0:-1]]\n",
    "y = (df.unacc == 'unacc').astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.94219653179190754,\n",
       " array([[ 5.41572917, -2.16359832, -1.9361105 ,  1.17767831,  0.22744353,\n",
       "         -0.07430495, -0.01479654, -2.73114337,  5.27330833, -1.22614461,\n",
       "         -0.51857301,  0.17152639,  1.66306697,  0.75924602, -0.97830128,\n",
       "         -0.88392402,  2.41899963,  1.22386666, -1.33869994, -1.0184942 ,\n",
       "          2.44934783]]))"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_enc = DictVectorizer(sparse=False).fit_transform(x.to_dict(orient='records'))\n",
    "train_i, test_i = train_test_split(range(0, len(x_enc)), test_size=.2)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x_enc[train_i], y[train_i])\n",
    "lr.score(x_enc[test_i], y[test_i]), lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1727, 21)\n",
      "0.942196531792 [[-0.07430495 -2.16359832  0.75924602 -0.97830128  0.17152639 -2.73114337\n",
      "   2.44934783  1.17767831 -1.9361105  -1.0184942  -0.51857301  2.41899963\n",
      "   0.22744353 -1.33869994  1.22386666 -1.22614461  1.66306697  5.41572917\n",
      "  -0.88392402  5.27330833 -0.01479654]]\n"
     ]
    }
   ],
   "source": [
    "b = 3000\n",
    "\n",
    "dfx = pd.DataFrame(hash_trick( x.values, x.columns, n=b ))\n",
    "dfx = dfx.fillna(0)\n",
    "print dfx.shape\n",
    "\n",
    "lr1 = LogisticRegression()\n",
    "lr1.fit(dfx.ix[train_i], y[train_i])\n",
    "print lr1.score(dfx.ix[test_i], y[test_i]), lr1.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(sorted(lr.coef_)) - np.array(sorted(lr1.coef_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
